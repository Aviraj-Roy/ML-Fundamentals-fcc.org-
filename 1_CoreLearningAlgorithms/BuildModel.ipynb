{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SepalLength  SepalWidth  PetalLenght  PetalWidth  Species\n",
      "0          6.4         2.8          5.6         2.2        2\n",
      "1          5.0         2.3          3.3         1.0        1\n",
      "2          4.9         2.5          4.5         1.7        2\n",
      "3          4.9         3.1          1.5         0.1        0\n",
      "4          5.7         3.8          1.7         0.3        0\n",
      "   SepalLength  SepalWidth  PetalLenght  PetalWidth\n",
      "0          6.4         2.8          5.6         2.2\n",
      "1          5.0         2.3          3.3         1.0\n",
      "2          4.9         2.5          4.5         1.7\n",
      "3          4.9         3.1          1.5         0.1\n",
      "4          5.7         3.8          1.7         0.3\n",
      "(120, 4)\n",
      "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalLenght', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(my_feature_columns)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241m.\u001b[39mEstimator\u001b[38;5;241m.\u001b[39mDNNClassifier(\n\u001b[0;32m     45\u001b[0m     feature_columns\u001b[38;5;241m=\u001b[39mmy_feature_columns,\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#Two Hidden Layers of 30 and 10 nodes respectively.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     hidden_units \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m#The model must choose between 3 classes\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     51\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     52\u001b[0m     input_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: input_fn(train, train_y, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     53\u001b[0m     steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#We include a lambda to avoid creating an inner function previously\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'estimator'"
     ]
    }
   ],
   "source": [
    "#tensorflow_version 2.x \n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf     \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLenght', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "#Constants defined to help us\n",
    "\n",
    "train_path = tf.keras.utils.get_file(\"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
    "test_path = tf.keras.utils.get_file(\"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "#We use \"keras\"(a module inside tensorflow) to grab our datasets and read them into a pandas dataframe\n",
    "\n",
    "print(train.head())\n",
    "train_y = train.pop('Species')\n",
    "test_y = test.pop('Species')\n",
    "print(train.head())\n",
    "\n",
    "print(train.shape)\n",
    "\n",
    "#Input Function\n",
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    #Convert inputs to a Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dict(features),labels)\n",
    "    \n",
    "    #shuffle and repeat if you are in training mode\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "        \n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# Feature columsn describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "print(my_feature_columns)\n",
    "\n",
    "#Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    #Two Hidden Layers of 30 and 10 nodes respectively.\n",
    "    hidden_units = [30, 10],\n",
    "    #The model must choose between 3 classes\n",
    "    n_classes=3)\n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
    "    steps = 5000\n",
    ")\n",
    "#We include a lambda to avoid creating an inner function previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = classifier.evaluate(input_fn=lambda: input_fn(test, test_y, training=False))\n",
    "print('\\nTest Set Accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(features, batch_size=256):\n",
    "    #Convert the inputs to a Dataset without labels\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "\n",
    "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
    "predict={}\n",
    "\n",
    "print(\"Please type numeric values as prompted.\")\n",
    "for feature in features:\n",
    "    valid = True\n",
    "    while valid:\n",
    "        val = input(feature+\": \")\n",
    "        if not val.isdigit(): valid = False\n",
    "        \n",
    "    predict[feature] = [float(val)]\n",
    "    \n",
    "predictions = classifier.predict(input_fn=lambda: input_fn(predict))\n",
    "for pred_dict in predictions:\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "    \n",
    "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
    "        SPECIES[class_id], 100*probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
